{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbilalnust/spam_sms_text_classification/blob/main/fcc_sms_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html,\n",
        "                                    '[%s]' % re.escape(string.punctuation),\n",
        "                                    '')"
      ],
      "metadata": {
        "id": "47sNsHAt1kRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows that become empty after standardization\n",
        "def clean_empty_messages(df):\n",
        "    cleaned = [custom_standardization(msg).numpy().decode(\"utf-8\").strip() for msg in df[\"message\"].values]\n",
        "    mask = [msg != \"\" for msg in cleaned]\n",
        "    return df[mask]\n"
      ],
      "metadata": {
        "id": "51Crs0BU1qQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data\n",
        "train_df = pd.read_csv(\"train-data.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n",
        "# Load validation data\n",
        "val_df = pd.read_csv(\"valid-data.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n",
        "\n",
        "# Drop empty messages\n",
        "train_df = train_df[train_df['message'].str.strip() != '']\n",
        "val_df = val_df[val_df['message'].str.strip() != '']\n",
        "\n",
        "# Encode labels: ham = 0, spam = 1\n",
        "train_df['label'] = train_df['label'].map({'ham': 0, 'spam': 1})\n",
        "val_df['label'] = val_df['label'].map({'ham': 0, 'spam': 1})\n",
        "train_df = clean_empty_messages(train_df)\n",
        "val_df = clean_empty_messages(val_df)\n",
        "\n",
        "# Extract messages and labels for train and validation sets\n",
        "train_texts = train_df[\"message\"].values\n",
        "train_labels = train_df[\"label\"].values\n",
        "val_texts = val_df[\"message\"].values\n",
        "val_labels = val_df[\"label\"].values"
      ],
      "metadata": {
        "id": "vMwCazoOt5oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "DV8wj5fS4Miy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Oversampling to balance spam and ham ---\n",
        "ham_df = train_df[train_df['label'] == 0]\n",
        "spam_df = train_df[train_df['label'] == 1]\n",
        "\n",
        "spam_oversampled = spam_df.sample(len(ham_df), replace=True, random_state=42)\n",
        "\n",
        "train_df = pd.concat([ham_df, spam_oversampled])\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_texts = train_df[\"message\"].values\n",
        "train_labels = train_df[\"label\"].values\n"
      ],
      "metadata": {
        "id": "j9oAv8wE6Nhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "qpCSo6Om6OoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text vectorization layer\n",
        "# Text Vectorization layer\n",
        "# Text Vectorization layer\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "vectorizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "# Adapt vectorizer on training text only\n",
        "vectorizer.adapt(train_texts)"
      ],
      "metadata": {
        "id": "Pt5EqLeuqTWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Prepare dataset\n",
        "raw_train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
        "raw_train_ds = raw_train_ds.batch(32)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "  return vectorizer(text), label\n",
        "\n",
        "# 3. Get a batch\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "\n",
        "# 4. First review and label\n",
        "first_review = text_batch[0].numpy().decode('utf-8')\n",
        "first_label = label_batch[0].numpy()\n",
        "\n",
        "print(\"Review:\", first_review)\n",
        "print(\"Label:\", \"ham\" if first_label == 0 else \"spam\")\n",
        "\n",
        "# 5. Vectorize first review\n",
        "vectorized_review, label = vectorize_text(first_review, first_label)\n",
        "print(\"Vectorized review:\", vectorized_review.numpy())"
      ],
      "metadata": {
        "id": "sU_hSNrnwOP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1287 ---> \",vectorizer.get_vocabulary()[2256])\n",
        "print(\" 313 ---> \",vectorizer.get_vocabulary()[4156])\n",
        "print('Vocabulary size: {}'.format(len(vectorizer.get_vocabulary())))"
      ],
      "metadata": {
        "id": "PIAz_jbgwVKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))      # val_texts, val_labels from val_df\n",
        "# raw_test_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels))"
      ],
      "metadata": {
        "id": "LVy_5yOmw1dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1287 ---> \", vectorizer.get_vocabulary()[7632])\n",
        "print(\" 313 ---> \", vectorizer.get_vocabulary()[2256])\n",
        "print('Vocabulary size: {}'.format(len(vectorizer.get_vocabulary())))\n"
      ],
      "metadata": {
        "id": "buTbH3xLrBDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create raw datasets\n",
        "raw_train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
        "raw_val_ds = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "# test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "0P2tvLghxnPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add batching, shuffling, and performance optimizations\n",
        "batch_size = 32\n",
        "train_ds = train_ds.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "277i-CUUyheF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=max_features + 1, output_dim=embedding_dim, input_length=sequence_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "6bJKRz_VxLnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "71LZDDBozBkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3neMUsJezdIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "metadata": {
        "id": "eHzNNGiOzqs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [],
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "    # Vectorize the input text, expand dims to make batch of 1\n",
        "    vectorized_text = vectorizer(tf.expand_dims(pred_text, 0))\n",
        "\n",
        "    # Get model prediction (probability between 0 and 1)\n",
        "    prediction_prob = model.predict(vectorized_text)[0][0]\n",
        "\n",
        "    # Convert probability to label\n",
        "    label = \"spam\" if prediction_prob >= 0.5 else \"ham\"\n",
        "\n",
        "    # Return as list [probability, label]\n",
        "    return [float(prediction_prob), label]\n",
        "\n",
        "\n",
        "pred_text = \"our new mobile video service is live. just install on your phone to start watching.\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {},
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}